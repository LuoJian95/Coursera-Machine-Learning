Week 2 内容框架

1.多变量线性回归 
  1.1 假设函数 h
  1.2 损失函数 J
  1.3 梯度下降算法 
  
2.梯度下降优化策略
  2.1 数据预处理
      2.1.1 feature scaling(思考为什么要feature scaling?)
      2.1.2 mean normalization
  2.2 学习率的调整
  
3.单变量多项式回归
  3.1 将单变量多项式回归转换为多变量线性回归

4.Normal Equation(正规方程)
  4.1 Normal Equation vs Gradient Descent
  4.2 Normal Equation Noninvertibility

5. x = A-1b 的梯度下降解法

6. 线性回归大作业
  6.1 cost function 的优化写法
  6.2 Gradient Descent 的简便写法
  6.3 theta 的两种解法比较
  6.4 feature normalization 中 bsxfun 函数的使用

